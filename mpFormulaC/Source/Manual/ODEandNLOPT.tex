
\chapter{Ordinary Differential Equations}
\label{OrdinaryDifferentialEquations} 

The procedures in this chapter are based on Boost.Numeric.Odeint (see \cite{boost_odeint}), a library for solving initial value problems (IVP) of ordinary differential equations. Mathematically, these problems are formulated as follows: 

$x'(t) = f(x,t), x(0) = x0. $

$x$ and $f$ can be vectors and the solution is some function $x(t)$ fulfilling both equations above. In the following we will refer to $x'(t)$ also $dxdt$ which is also our notation for the derivative in the source code. 

Ordinary differential equations occur nearly everywhere in natural sciences. For example, the whole Newtonian mechanics are described by second order differential equations. Be sure, you will find them in every discipline. They also occur if partial differential equations (PDEs) are discretized. Then, a system of coupled ordinary differential occurs, sometimes also referred as lattices ODEs. 

Numerical approximations for the solution $x(t)$ are calculated iteratively. The easiest algorithm is the Euler scheme, where starting at $x(0)$ one finds $x(dt) = x(0) + dt f(x(0),0)$. Now one can use $x(dt)$ and obtain $x(2dt)$ in a similar way and so on. The Euler method is of order 1, that means the error at each step is $\approx dt2$. This is, of course, not very satisfactory, which is why the Euler method is rarely used for real life problems and serves just as illustrative example. 

The main focus of odeint is to provide numerical methods implemented in a way where the algorithm is completely independent on the data structure used to represent the state x. In doing so, odeint is applicable for a broad variety of situations and it can be used with many other libraries. Besides the usual case where the state is defined as a std::vector or a boost::array, we provide native support for the following libraries: 

General Literature includes:

General information about numerical integration of ordinary differential equations: 

\cite{NumericalRecipes_2007}

\cite{Hairer_2009}

\cite{Hairer_2010}



Symplectic integration of numerical integration: 

\cite{Hairer_2006}

\cite{Leimkuhler_2005}



Special symplectic methods: 

\cite{Yoshida_1990}


\cite{McLachlan_1995}




Special systems: 

Fermi-Pasta-Ulam nonlinear lattice oscillations %at http://www.scholarpedia.org/article/Fermi-Pasta-Ulam_nonlinear_lattice_oscillations

\cite{Pikovsky_2001}



\section{Defining the ODE System}
\label{IntroductionODE}

The routines solve the general n-dimensional first-order system,
\begin{equation}
\frac{dy_i(t)}{dt} = f_i(t, y_1(t), . . . y_n(t))
\end{equation}
for $i = 1, . . . , n$. The stepping functions rely on the vector of derivatives $f_i$ and the Jacobian
matrix, $J_{ij} =\partial f_i(t, y(t))/\partial y_j$. A system of equations is defined using the 
system datatype.

\section{Stepping Functions}
\label{SteppingFunctionsODE}

Solving ordinary differential equation numerically is usually done iteratively, that is a given state of an ordinary differential equation is iterated forward $x(t) -> x(t+dt) -> x(t+2dt)$. The steppers in odeint perform one single step. The most general stepper type is described by the Stepper concept. The stepper concepts of odeint are described in detail in section Concepts, here we briefly present the mathematical and numerical details of the steppers. The Stepper has two versions of the do\_step method, one with an in-place transform of the current state and one with an out-of-place transform: 

do\_step( sys , inout , t , dt ) 

do\_step( sys , in , t , out , dt ) 

The first parameter is always the system function - a function describing the ODE. In the first version the second parameter is the step which is here updated in-place and the third and the fourth parameters are the time and step size (the time step). After a call to do\_step the state inout is updated and now represents an approximate solution of the ODE at time t+dt. In the second version the second argument is the state of the ODE at time t, the third argument is t, the fourth argument is the approximate solution at time t+dt which is filled by do\_step and the fifth argument is the time step. Note that these functions do not change the time t. 

System functions 

Up to now, we have nothing said about the system function. This function depends on the stepper. For the explicit Runge-Kutta steppers this function can be a simple callable object hence a simple (global) C-function or a functor. The parameter syntax is $sys( x , dxdt , t )$ and it is assumed that it calculates $dx/dt = f(x,t)$. The function structure in most cases looks like: 


void sys( const state\_type \& /*x*/ , state\_type \& /*dxdt*/ , const double /*t*/ )
{
    // ...
}


Other types of system functions might represent Hamiltonian systems or systems which also compute the Jacobian needed in implicit steppers. For information which stepper uses which system function see the stepper table below. It might be possible that odeint will introduce new system types in near future. Since the system function is strongly related to the stepper type, such an introduction of a new stepper might result in a new type of system function. 

Explicit steppers 
A first specialization are the explicit steppers. Explicit means that the new state of the ode can be computed explicitly from the current state without solving implicit equations. Such steppers have in common that they evaluate the system at time t such that the result of f(x,t) can be passed to the stepper. In odeint, the explicit stepper have two additional methods 


Which steppers should be used in which situation 

odeint provides a quite large number of different steppers such that the user is left with the question of which stepper fits his needs. Our personal recommendations are: 

runge\_kutta\_dopri5 is maybe the best default stepper. It has step size control as well as dense-output functionality. Simple create a dense-output stepper by make\_dense\_output( 1.0e-6 , 1.0e-5 , runge\_kutta\_dopri5< state\_type >() ). 
runge\_kutta4 is a good stepper for constant step sizes. It is widely used and very well known. If you need to create artificial time series this stepper should be the first choice. 
'runge\_kutta\_fehlberg78' is similar to the 'runge\_kutta4' with the advantage that it has higher precision. It can also be used with step size control. 
adams\_bashforth\_moulton is very well suited for ODEs where the r.h.s. is expensive (in terms of computation time). It will calculate the system function only once during each step. 



\subsection{Explicit Euler}
\label{Explicit Euler}

In mathematics and computational science, the Euler method is a first-order numerical procedure for solving ordinary differential equations (ODEs) with a given initial value. It is the most basic explicit method for numerical integration of ordinary differential equations and is the simplest Runge–Kutta method. The Euler method is named after Leonhard Euler, who treated it in his book Institutionum calculi integralis (published 1768–70).[1]

The Euler method is a first-order method, which means that the local error (error per step) is proportional to the square of the step size, and the global error (error at a given time) is proportional to the step size. The Euler method often serves as the basis to construct more complicated methods.





\subsection{Modified Midpoint}
\label{Modified Midpoint}

In numerical analysis, a branch of applied mathematics, the midpoint method is a one-step method for numerically solving the differential equation,

$y'(t)=f(t,y(t), y(t_0)=y_0$

and is given by the formula

$y_{n+1}=y_n + h f(t_n+h/2, y_n+ (h/2) f(t_n, y_n),$

for $n=0,1,2,...$  Here, $h$ is the step size - a small positive number, $t_n=t_0+nh$ and $y_n$ is the computed approximate value of $y(t_n)$. The midpoint method is also known as the modified Euler method.[1]

The name of the method comes from the fact that in the formula above the function $f$ is evaluated at $t=t_n+h/2$ which is the midpoint between $t_n$ at which the value of $y(t)$ is known and $t_{n+1}$ at which the value of $y(t)$ needs to be found.

The local error at each step of the midpoint method is of order $O(h^3)$, giving a global error of order $O(h^2)$. Thus, while more computationally intensive than Euler's method, the midpoint method generally gives more accurate results.

The method is an example of a class of higher-order methods known as Runge-Kutta methods.




\subsection{Runge-Kutta 4}
\label{RungeKutta4}

In numerical analysis, the Runge–Kutta methods are an important family of implicit and explicit iterative methods, which are used in temporal discretization for the approximation of solutions of ordinary differential equations. These techniques were developed around 1900 by the German mathematicians C. Runge and M. W. Kutta.

See the article on numerical methods for ordinary differential equations for more background and other methods. See also List of Runge–Kutta methods.

One member of the family of Runge–Kutta methods is often referred to as "RK4", "classical Runge–Kutta method" or simply as "the Runge–Kutta method".

Let an initial value problem be specified as follows.


Here, y is an unknown function (scalar or vector) of time t which we would like to approximate; we are told that , the rate at which y changes, is a function of t and of y itself. At the initial time  the corresponding y-value is . The function f and the data ,  are given.




\subsection{Cash-Karp}
\label{CashKarp}

In numerical analysis, the Cash–Karp method is a method for solving ordinary differential equations (ODEs). It was proposed by Professor Jeff R. Cash [1] from Imperial College London and Alan H. Karp from IBM Scientific Center. The method is a member of the Runge–Kutta family of ODE solvers. More specifically, it uses six function evaluations to calculate fourth- and fifth-order accurate solutions. The difference between these solutions is then taken to be the error of the (fourth order) solution. This error estimate is very convenient for adaptive stepsize integration algorithms. Other similar integration methods are Fehlberg (RKF) and Dormand–Prince (RKDP).

J. R. Cash, A. H. Karp. "A variable order Runge-Kutta method for initial value problems with rapidly varying right-hand sides", ACM Transactions on Mathematical Software 16: 201-222, 1990. doi:10.1145/79505.79507.

Shampine, Lawrence F. (1986), "Some Practical Runge-Kutta Formulas", Mathematics of Computation (American Mathematical Society) 46 (173): 135–150, doi:10.2307/2008219, JSTOR 2008219 .



\subsection{Dormand-Prince 5}
\label{DormandPrince}

In numerical analysis, the Dormand–Prince method, or DOPRI method, is an explicit method for solving ordinary differential equations \citep{Dormand1980}. The method is a member of the Runge–Kutta family of ODE solvers. More specifically, it uses six function evaluations to calculate fourth- and fifth-order accurate solutions. The difference between these solutions is then taken to be the error of the (fourth-order) solution. This error estimate is very convenient for adaptive stepsize integration algorithms. Other similar integration methods are Fehlberg (RKF) and Cash–Karp (RKCK).

The Dormand–Prince method has seven stages, but it uses only six function evaluations per step because it has the FSAL (First Same As Last) property: the last stage is evaluated at the same point as the first stage of the next step. Dormand and Prince choose the coefficients of their method to minimize the error of the fifth-order solution. This is the main difference with the Fehlberg method, which was constructed so that the fourth-order solution has a small error. For this reason, the Dormand–Prince method is more suitable when the higher-order solution is used to continue the integration, a practice known as local extrapolation (Shampine 1986; Hairer, Nørsett \& Wanner 2008, pp. 178–179).

Dormand–Prince is currently the default method in MATLAB and GNU Octave's ode45 solver and is the default choice for the Simulink's model explorer solver. A Fortran free software implementation of the algorithm called DOPRI5 is also available.[1]




\subsection{Fehlberg 78}
In mathematics, the Runge–Kutta–Fehlberg method (or Fehlberg method) is an algorithm in numerical analysis for the numerical solution of ordinary differential equations. It was developed by the German mathematician Erwin Fehlberg and is based on the large class of Runge–Kutta methods.

The novelty of Fehlberg's method is that it is an embedded method from the Runge-Kutta family, meaning that identical function evaluations are used in conjunction with each other to create methods of varying order and similar error constants. The method presented in Fehlberg's 1969 paper has been dubbed the RKF45 method, and is a method of order O(h4) with an error estimator of order O(h5).[1] By performing one extra calculation, the error in the solution can be estimated and controlled by using the higher-order embedded method that allows for an adaptive stepsize to be determined automatically.

Erwin Fehlberg (1970). "Klassische Runge-Kutta-Formeln vierter und niedrigerer Ordnung mit Schrittweiten-Kontrolle und ihre Anwendung auf Wärmeleitungsprobleme," Computing (Arch. Elektron. Rechnen), vol. 6, pp. 61–71. doi:10.1007/BF02241732




\subsection{Adams-Bashforth}
\label{Adams-Bashforth}

Three families of linear multistep methods are commonly used: Adams–Bashforth methods, Adams–Moulton methods, and the backward differentiation formulas (BDFs).

Adams–Bashforth methods[edit]The Adams–Bashforth methods are explicit methods. The coefficients are  and , while the  are chosen such that the methods has order s (this determines the methods uniquely).

The Adams–Bashforth methods with s = 1, 2, 3, 4, 5 are (Hairer, Nørsett \& Wanner 1993, §III.1; Butcher 2003, p. 103):




\subsection{Adams-Moulton}
\label{Adams-Moulton}

The Adams–Moulton methods are similar to the Adams–Bashforth methods in that they also have  and . Again the b coefficients are chosen to obtain the highest order possible. However, the Adams–Moulton methods are implicit methods. By removing the restriction that , an s-step Adams–Moulton method can reach order , while an s-step Adams–Bashforth methods has only order s.

The Adams–Moulton methods with s = 0, 1, 2, 3, 4 are (Hairer, Nørsett \& Wanner 1993, §III.1; Quarteroni, Sacco \& Saleri 2000):




\subsection{Adams-Bashforth-Moulton}
\label{Adams-Bashforth-Moulton}

The methods of Euler, Heun, Taylor and Runge-Kutta are called single-step methods because they use only the information from one previous point to compute the successive point, that is, only the initial point    is used to compute    and in general    is needed to compute  .  After several points have been found it is feasible to use several prior points in the calculation.  The Adams-Bashforth-Moulton method uses   in the calculation of .  This method is not self-starting;  four initial points  , , ,  and  must be given in advance in order to generate the points .  

    A desirable feature of a multistep method is that the local truncation error (L. T. E.) can be determined and a correction term can be included, which improves the accuracy of the answer at each step.  Also, it is possible to determine if the step size is small enough to obtain an accurate value for  , yet large enough so that unnecessary and time-consuming calculations are eliminated.  If the code for the subroutine is fine-tuned, then the combination of a  predictor and corrector requires only two function evaluations of  f(t,y)  per step. 

See also: http://mathfaculty.fullerton.edu/mathews//n2003/AdamsBashforthMod.html


\subsection{Controlled Runge-Kutta}
\label{Controlled Runge-Kutta}

\lipsum[2]


\subsection{Dense Output Runge-Kutta}
\label{Dense Output Runge-Kutta}

\lipsum[2]


\subsection{Bulirsch-Stoer}
\label{Bulirsch-Stoer}

\lipsum[4]


\subsection{Bulirsch-Stoer Dense Output}
\label{Bulirsch-Stoer Dense Output}

\lipsum[4]



\subsection{Implicit Euler}
\label{Implicit Euler}

\lipsum[2]


\subsection{Rosenbrock 4}
\label{Rosenbrock 4}

\lipsum[2]


\subsection{Controlled Rosenbrock 4}
\label{Controlled Rosenbrock 4}

\lipsum[2]


\subsection{Dense Output Rosenbrock 4}
\label{Dense Output Rosenbrock 4}

\lipsum[2]


\subsection{Symplectic Euler}
\label{Symplectic Euler}

\lipsum[2]


\subsection{Symplectic RKN McLachlan}
\label{Symplectic RKN McLachlan}

\lipsum[2]


\section{Integrate functions: Evolution}
\label{EvolutionODE}

Integrate functions perform the time evolution of a given ODE from some starting time t0 to a given end time t1 and starting at state x0 by subsequent calls of a given stepper's do\_step function. 

Additionally, the user can provide an observer to analyze the state during time evolution. There are five different integrate functions which have different strategies on when to call the observer function during integration. 

All of the integrate functions except integrate\_n\_steps can be called with any stepper following one of the stepper concepts: Stepper , Error Stepper , Controlled Stepper , Dense Output Stepper. Depending on the abilities of the stepper, the integrate functions make use of step-size control or dense output. 








\chapter{Nonlinear Root-finding, Minimization and Optimization}
\label{NonlinearOptimization} % So I can \ref{altrings3} later.
%\lipsum[2-3]



\section{One-Dimensional Root-finding}
\label{OneDimensionalRootfinding}

The root bracketing algorithms described in this section require an initial interval which is
guaranteed to contain a root—if a and b are the endpoints of the interval then f(a) must
differ in sign from f(b). This ensures that the function crosses zero at least once in the
interval. If a valid initial interval is used then these algorithm cannot fail, provided the
function is well-behaved.
Note that a bracketing algorithm cannot find roots of even degree, since these do not
cross the x-axis.


\subsection{Bisection}
bisection [Solver]
The bisection algorithm is the simplest method of bracketing the roots of a function.
It is the slowest algorithm provided by the library, with linear convergence.
On each iteration, the interval is bisected and the value of the function at the midpoint
is calculated. The sign of this value is used to determine which half of the interval does
not contain a root. That half is discarded to give a new, smaller interval containing
the root. This procedure can be continued indefinitely until the interval is sufficiently
small.
At any time the current estimate of the root is taken as the midpoint of the interval.


\subsection{False Position}
falsepos [Solver]
The false position algorithm is a method of finding roots based on linear interpolation.
Its convergence is linear, but it is usually faster than bisection.
On each iteration a line is drawn between the endpoints (a, f(a)) and (b, f(b)) and
the point where this line crosses the x-axis taken as a “midpoint”. The value of the
function at this point is calculated and its sign is used to determine which side of the
interval does not contain a root. That side is discarded to give a new, smaller interval
containing the root. This procedure can be continued indefinitely until the interval
is sufficiently small.
The best estimate of the root is taken from the linear interpolation of the interval on
the current iteration.








\subsection{Brent-Dekker}
brent [Solver]
The Brent-Dekker method (referred to here as Brent’s method) combines an interpolation
strategy with the bisection algorithm. This produces a fast algorithm which is
still robust.
On each iteration Brent’s method approximates the function using an interpolating
curve. On the first iteration this is a linear interpolation of the two endpoints. For
subsequent iterations the algorithm uses an inverse quadratic fit to the last three
points, for higher accuracy. The intercept of the interpolating curve with the x-axis
is taken as a guess for the root. If it lies within the bounds of the current interval
then the interpolating point is accepted, and used to generate a smaller interval. If
the interpolating point is not accepted then the algorithm falls back to an ordinary
bisection step.
The best estimate of the root is taken from the most recent interpolation or bisection.


\subsection{Newton}
Root Finding Algorithms using Derivatives
The root polishing algorithms described in this section require an initial guess for the
location of the root. There is no absolute guarantee of convergence—the function must be
suitable for this technique and the initial guess must be sufficiently close to the root for it
to work. When these conditions are satisfied then convergence is quadratic.
These algorithms make use of both the function and its derivative.

newton [Derivative Solver]
Newton’s Method is the standard root-polishing algorithm. The algorithm begins
with an initial guess for the location of the root. On each iteration, a line tangent to
the function f is drawn at that position. The point where this line crosses the x-axis
becomes the new guess. The iteration is defined by the following sequence,

$xi+1 = xi − f(xi) f′(xi)$

Newton’s method converges quadratically for single roots, and linearly for multiple
roots.


\subsection{Secant}
secant [Derivative Solver]
The secant method is a simplified version of Newton’s method which does not require
the computation of the derivative on every step.
On its first iteration the algorithm begins with Newton’s method, using the derivative
to compute a first step,

$x1 = x0 − f(x0) f′(x0)$

Subsequent iterations avoid the evaluation of the derivative by replacing it with a
numerical estimate, the slope of the line through the previous two points,

$i+1 = xi − f(xi) f′ est$

where 

$f′est =f(xi) − f(xi−1)xi − xi−1$

When the derivative does not change significantly in the vicinity of the root the
secant method gives a useful saving. Asymptotically the secant method is faster than
Newton’s method whenever the cost of evaluating the derivative is more than 0.44
times the cost of evaluating the function itself. As with all methods of computing a
numerical derivative the estimate can suffer from cancellation errors if the separation
of the points becomes too small.
On single roots, the method has a convergence of order (1 + √5)/2 (approximately
1.62). It converges linearly for multiple roots.




\subsection{Steffenson}
steffenson [Derivative Solver]
The Steffenson Method1 provides the fastest convergence of all the routines. It combines
the basic Newton algorithm with an Aitken “delta-squared” acceleration. If the
Newton iterates are xi then the acceleration procedure generates a new sequence Ri,

$Ri = xi − (xi+1 − xi)2 (xi+2 − 2xi+1 + xi)$

which converges faster than the original sequence under reasonable conditions. The
new sequence requires three terms before it can produce its first value so the method
returns accelerated values on the second and subsequent iterations. On the first
iteration it returns the ordinary Newton estimate. The Newton iterate is also returned
if the denominator of the acceleration term ever becomes zero.
As with all acceleration procedures this method can become unstable if the function
is not well-behaved.



\section{One-Dimensional Minimization}



\subsection{Minimization: goldensection}
goldensection [Minimizer]
The golden section algorithm is the simplest method of bracketing the minimum of a
function. It is the slowest algorithm provided by the library, with linear convergence.
On each iteration, the algorithm first compares the subintervals from the endpoints to
the current minimum. The larger subinterval is divided in a golden section (using the
famous ratio $(3 − \sqrt{5})/2 = 0.3189660. . . )$ and the value of the function at this new
point is calculated. The new value is used with the constraint $f(a′) > f(x′) < f(b′)$
to a select new interval containing the minimum, by discarding the least useful point.
This procedure can be continued indefinitely until the interval is sufficiently small.
Choosing the golden section as the bisection ratio can be shown to provide the fastest
convergence for this type of algorithm.


\subsection{Minimization: Brent-Dekker}
brent [Minimizer]
The Brent minimization algorithm combines a parabolic interpolation with the golden
section algorithm. This produces a fast algorithm which is still robust.
The outline of the algorithm can be summarized as follows: on each iteration Brent’s
method approximates the function using an interpolating parabola through three
existing points. The minimum of the parabola is taken as a guess for the minimum.
If it lies within the bounds of the current interval then the interpolating point is
accepted, and used to generate a smaller interval. If the interpolating point is not
accepted then the algorithm falls back to an ordinary golden section step. The full
details of Brent’s method include some additional checks to improve convergence.

\subsection{Minimization: Brent-Dekker-Gill-Murray}
quad\_golden [Minimizer]
This is a variant of Brent’s algorithm which uses the safeguarded step-length algorithm
of Gill and Murray.



\section{Procedures based on MINPACK}


Reference for  MINPACK: \cite{More_1980}.

Detailed Description

\#include <unsupported/Eigen/NonLinearOptimization>

This module provides implementation of two important algorithms in non linear optimization. In both cases, we consider a system of non linear functions. Of course, this should work, and even work very well if those functions are actually linear. But if this is so, you should probably better use other methods more fitted to this special case.

One algorithm allows to find an extremum of such a system (Levenberg Marquardt algorithm) and the second one is used to find a zero for the system (Powell hybrid "dogleg" method).

This code is a port of MINPACK . Minpack is a very famous, old, robust and well-reknown package, written in fortran. Those implementations have been carefully tuned, tested, and used for several decades.

The original fortran code was automatically translated using f2c in C, then c++, and then cleaned by several different authors. 

Finally, we ported this code to Eigen, creating classes and API coherent with Eigen. When possible, we switched to Eigen implementation, such as most linear algebra (vectors, matrices, stable norms).

Doing so, we were very careful to check the tests we setup at the very beginning, which ensure that the same results are found.


Tests

The tests are placed in the file unsupported/test/NonLinear.cpp.

There are two kinds of tests : those that come from examples bundled with cminpack. They guaranty we get the same results as the original algorithms (value for 'x', for the number of evaluations of the function, and for the number of evaluations of the jacobian if ever).

Other tests were added by myself at the very beginning of the process and check the results for levenberg-marquardt using the reference data on NIST. Since then i've carefully checked that the same results were obtained when modifiying the code. Please note that we do not always get the exact same decimals as they do, but this is ok : they use 128bits float, and we do the tests using the C type 'double', which is 64 bits on most platforms (x86 and amd64, at least). I've performed those tests on several other implementations of levenberg-marquardt, and (c)minpack performs VERY well compared to those, both in accuracy and speed.

The documentation for running the tests is on the wiki http://eigen.tuxfamily.org/index.php?title=Tests
API : overview of methods

Both algorithms can use either the jacobian (provided by the user) or compute an approximation by themselves (actually using Eigen Numerical differentiation module). The part of API referring to the latter use 'NumericalDiff' in the method names (exemple: LevenbergMarquardt.minimizeNumericalDiff() )

The methods LevenbergMarquardt.lmder1()/lmdif1()/lmstr1() and HybridNonLinearSolver.hybrj1()/hybrd1() are specific methods from the original minpack package that you probably should NOT use until you are porting a code that was previously using minpack. They just define a 'simple' API with default values for some parameters.

All algorithms are provided using Two APIs :

one where the user inits the algorithm, and uses '*OneStep()' as much as he wants : this way the caller have control over the steps

one where the user just calls a method (optimize() or solve()) which will handle the loop: init + loop until a stop condition is met. Those are provided for convenience.

As an example, the method LevenbergMarquardt::minimize() is implemented as follow :

\lstset{language={C++}}
\begin{lstlisting}
Status LevenbergMarquardt<FunctorType,Scalar>::minimize(FVectorType &x, const int mode)
{
Status status = minimizeInit(x, mode);
do {
status = minimizeOneStep(x, mode);
} while (status==Running);
return status;
}
\end{lstlisting}


The easiest way to understand how to use this module is by looking at the many examples in the file unsupported/test/NonLinearOptimization.cpp. 

\subsection{Multidimensional Rootfinding: Powell Hybrid}
This is a modified version of Powell’s Hybrid method as implemented in the hybrj
algorithm in minpack.  The Hybrid algorithm retains the fast convergence of Newton’s
method but will also reduce the residual when Newton’s method is unreliable.
The algorithm uses a generalized trust region to keep each step under control. In order
to be accepted a proposed new position $x′$ must satisfy the condition $|D(x′ −x)| < \delta$,
where $D$ is a diagonal scaling matrix and $\delta$ is the size of the trust region. The
components of $D$ are computed internally, using the column norms of the Jacobian
to estimate the sensitivity of the residual to each component of $x$. This improves the
behavior of the algorithm for badly scaled functions.
On each iteration the algorithm first determines the standard Newton step by solving
the system $Jdx = −f$. If this step falls inside the trust region it is used as a trial step
in the next stage. If not, the algorithm uses the linear combination of the Newton
and gradient directions which is predicted to minimize the norm of the function while
\begin{equation}
	dx = −\alpha J^{−1}f(x) − \beta\nabla|f(x)|^2.
\end{equation}
staying inside the trust region,

This combination of Newton and gradient directions is referred to as a dogleg step.
The proposed step is now tested by evaluating the function at the resulting point, $x′$.
If the step reduces the norm of the function sufficiently then it is accepted and size of
the trust region is increased. If the proposed step fails to improve the solution then
the size of the trust region is decreased and another trial step is computed.
The speed of the algorithm is increased by computing the changes to the Jacobian approximately,
using a rank-1 update. If two successive attempts fail to reduce the residual
then the full Jacobian is recomputed. The algorithm also monitors the progress
of the solution and returns an error if several steps fail to make any improvement.







\subsection{Nonlinear LeastSquares: Levenberg-Marquardt }
The minimization algorithms described in this section make use of both the function and
its derivative. They require an initial guess for the location of the minimum. There is no
absolute guarantee of convergence—the function must be suitable for this technique and
the initial guess must be sufficiently close to the minimum for it to work.


lmsder [Derivative Solver]
This is a robust and efficient version of the Levenberg-Marquardt algorithm as implemented
in the scaled lmder routine in minpack. Minpack was written by Jorge
J. Mor´e, Burton S. Garbow and Kenneth E. Hillstrom.


The algorithm uses a generalized trust region to keep each step under control. In order
to be accepted a proposed new position $x′$ must satisfy the condition $|D(x′ −x)| < \delta$,
where D is a diagonal scaling matrix and $\delta$ is the size of the trust region. The
components of D are computed internally, using the column norms of the Jacobian
to estimate the sensitivity of the residual to each component of x. This improves the
behavior of the algorithm for badly scaled functions.


On each iteration the algorithm attempts to minimize the linear system $|F + Jp|$ subject to the constraint $|Dp| < $. The solution to this constrained linear system is found using the Levenberg-Marquardt method.

The proposed step is now tested by evaluating the function at the resulting point,
$x′$. If the step reduces the norm of the function sufficiently, and follows the predicted
behavior of the function within the trust region, then it is accepted and the size of
the trust region is increased. If the proposed step fails to improve the solution, or
differs significantly from the expected behavior within the trust region, then the size
of the trust region is decreased and another trial step is computed.
The algorithm also monitors the progress of the solution and returns an error if the
changes in the solution are smaller than the machine precision. The possible error
codes are,






\section{Procedures based on NLOPT: Overview}

Reference to NLOPT is  is \cite{Johnson2012}


Nomenclature

Each algorithm in NLopt is identified by a named constant, which is passed to the NLopt routines in the various languages in order to select a particular algorithm. These constants are mostly of the form NLOPT\_{G,L}{N,D}\_xxxx, where G/L denotes global/local optimization and N/D denotes derivative-free/gradient-based algorithms, respectively. 

For example, the NLOPT\_LN\_COBYLA constant refers to the COBYLA algorithm (described below), which is a local (L) derivative-free (N) optimization algorithm. 

Two exceptions are the MLSL and augmented Lagrangian algorithms, denoted by NLOPT\_G\_MLSL and NLOPT\_AUGLAG, since whether or not they use derivatives (and whether or not they are global, in AUGLAG's case) is determined by what subsidiary optimization algorithm is specified. 

Many of the algorithms have several variants, which are grouped together below. 

Comparing algorithms

For any given optimization problem, it is a good idea to compare several of the available algorithms that are applicable to that problem—in general, one often finds that the "best" algorithm strongly depends upon the problem at hand. 

However, comparing algorithms requires a little bit of care because the function-value/parameter tolerance tests are not all implemented in exactly the same way for different algorithms. So, for example, the same fractional $10^{−4}$ tolerance on the function value might produce a much more accurate minimum in one algorithm compared to another, and matching them might require some experimentation with the tolerances. 

Instead, a more fair and reliable way to compare two different algorithms is to run one until the function value is converged to some value fA, and then run the second algorithm with the $minf_max$ termination test set to $minf_max=fA$. That is, ask how long it takes for the two algorithms to reach the same function value. 

Better yet, run some algorithm for a really long time until the minimum fM is located to high precision. Then run the different algorithms you want to compare with the termination test: $minf_max=fM+\Delta f$. That is, ask how long it takes for the different algorithms to obtain the minimum to within an absolute tolerance $\Delta f$, for some $\Delta f$. (This is totally different from using the $ftol_abs$ termination test, because the latter uses only a crude estimate of the error in the function values, and moreover the estimate varies between algorithms.) 




\section{NLOPT: Global optimization}
All of the global-optimization algorithms currently require you to specify bound constraints on all the optimization parameters. Of these algorithms, only ISRES and ORIG\_DIRECT support nonlinear inequality constraints, and only ISRES supports nonlinear equality constraints. (However, any of them can be applied to nonlinearly constrained problems by combining them with the augmented Lagrangian method below.) 

Something you should consider is that, after running the global optimization, it is often worthwhile to then use the global optimum as a starting point for a local optimization to "polish" the optimum to a greater accuracy. (Many of the global optimization algorithms devote more effort to searching the global parameter space than in finding the precise position of the local optimum accurately.) 




\subsection{DIRECT and DIRECT-L}
DIRECT is the DIviding RECTangles algorithm for global optimization, described in \cite{Jones_1993}

and DIRECT-L is the "locally biased" variant proposed by \cite{Gablonsky_2001}

These is are deterministic-search algorithms based on systematic division of the search domain into smaller and smaller hyperrectangles. The Gablonsky version makes the algorithm "more biased towards local search" so that it is more efficient for functions without too many local minima. NLopt contains several implementations of both of these algorithms. I would tend to try NLOPT\_GN\_DIRECT\_L first; YMMV. 

First, it contains a from-scratch re-implementation of both algorithms, specified by the constants NLOPT\_GN\_DIRECT and NLOPT\_GN\_DIRECT\_L, respectively. 

Second, there is a slightly randomized variant of DIRECT-L, specified by NLOPT\_GLOBAL\_DIRECT\_L\_RAND, which uses some randomization to help decide which dimension to halve next in the case of near-ties. 

The DIRECT and DIRECT-L algorithms start by rescaling the bound constraints to a hypercube, which gives all dimensions equal weight in the search procedure. If your dimensions do not have equal weight, e.g. if you have a "long and skinny" search space and your function varies at about the same speed in all directions, it may be better to use unscaled variants of these algorthms, which are specified as NLOPT\_GLOBAL\_DIRECT\_NOSCAL, NLOPT\_GLOBAL\_DIRECT\_L\_NOSCAL, and NLOPT\_GLOBAL\_DIRECT\_L\_RAND\_NOSCAL, respectively. However, the unscaled variations make the most sense (if any) with the original DIRECT algorithm, since the design of DIRECT-L to some extent relies on the search region being a hypercube (which causes the subdivided hyperrectangles to have only a small set of side lengths). 

Finally, NLopt also includes separate implementations based on the original Fortran code by Gablonsky et al. (1998-2001), which are specified as NLOPT\_GN\_ORIG\_DIRECT and NLOPT\_GN\_ORIG\_DIRECT\_L. These implementations have a number of hard-coded limitations on things like the number of function evaluations; I removed several of these limitations, but some remain. On the other hand, there seem to be slight differences between these implementations and mine; most of the time, the performance is roughly similar, but occasionally Gablonsky's implementation will do significantly better than mine or vice versa. 

Most of the above algorithms only handle bound constraints, and in fact require finite bound constraints (they are not applicable to unconstrained problems). They do not handle arbitrary nonlinear constraints. However, the ORIG versions by Gablonsky et al. include some support for arbitrary nonlinear inequality constraints. 



\subsection{Controlled Random Search (CRS) with local mutation}
My implementation of the "controlled random search" (CRS) algorithm (in particular, the CRS2 variant) with the "local mutation" modification, as defined by: 
\cite{Kaelo_2006}. 

The original CRS2 algorithm was described by: \cite{Price_1978, Price_1983} 


The CRS algorithms are sometimes compared to genetic algorithms, in that they start with a random "population" of points, and randomly "evolve" these points by heuristic rules. In this case, the "evolution" somewhat resembles a randomized Nelder-Mead algorithm. The published results for CRS seem to be largely empirical; limited analytical results about its convergence were derived in \cite{Hendrix_2001}


The initial population size for CRS defaults to 10×(n+1) in n dimensions, but this can be changed with the nlopt\_set\_stochastic\_population function; the initial population must be at least n+1. 

Only bound-constrained problems are supported by this algorithm. 

CRS2 with local mutation is specified in NLopt as NLOPT\_GN\_CRS2\_LM. 




\subsection{MLSL (Multi-Level Single-Limkage)}
This is my implementation of the "Multi-Level Single-Linkage" (MLSL) algorithm for global optimization by a sequence of local optimizations from random starting points, proposed by: \cite{RinnooyKan_1987a, RinnooyKan_1987b}


We also include a modification of MLSL use a Sobol' low-discrepancy sequence (LDS) instead of pseudorandom numbers, which was argued to improve the convergence rate by: \cite{Kucherenko_2005}


In either case, MLSL is a "multistart" algorithm: it works by doing a sequence of local optimizations (using some other local optimization algorithm) from random or low-discrepancy starting points. MLSL is distinguished, however by a "clustering" heuristic that helps it to avoid repeated searches of the same local optima, and has some theoretical guarantees of finding all local optima in a finite number of local minimizations. 

The local-search portion of MLSL can use any of the other algorithms in NLopt, and in particular can use either gradient-based (D) or derivative-free algorithms (N) The local search uses the derivative/nonderivative algorithm set by nlopt\_opt\_set\_local\_optimizer. 

LDS-based MLSL with is specified as NLOPT\_G\_MLSL\_LDS, while the original non-LDS original MLSL (using pseudo-random numbers, currently via the Mersenne twister algorithm) is indicated by NLOPT\_G\_MLSL. In both cases, you must specify the local optimization algorithm (which can be gradient-based or derivative-free) via nlopt\_opt\_set\_local\_optimizer. 

Note: If you do not set a stopping tolerance for your local-optimization algorithm, MLSL defaults to ftol\_rel=10−15 and xtol\_rel=10−7 for the local searches. Note that it is perfectly reasonable to set a relatively large tolerance for these local searches, run MLSL, and then at the end run another local optimization with a lower tolerance, using the MLSL result as a starting point, to "polish off" the optimum to high precision. 

By default, each iteration of MLSL samples 4 random new trial points, but this can be changed with the nlopt\_set\_population function. 

Only bound-constrained problems are supported by this algorithm. 




\subsection{StoGO}
This is an algorithm adapted from the code downloaded from 

StoGO global optimization library (link broken as of Nov. 2009, and the software seems absent from the author's web site) 
by Madsen et al. StoGO is a global optimization algorithm that works by systematically dividing the search space (which must be bound-constrained) into smaller hyper-rectangles via a branch-and-bound technique, and searching them by a gradient-based local-search algorithm (a BFGS variant), optionally including some randomness (hence the "Sto", which stands for "stochastic" I believe). 

StoGO is written in C++, which means that it is only included when you compile the C++ algorithms enabled, in which case (on Unix) you must link to -lnlopt\_cxx instead of -lnlopt. 

StoGO is specified within NLopt by NLOPT\_GD\_STOGO, or NLOPT\_GD\_STOGO\_RAND for the randomized variant. 

Some references on StoGO are: \cite{Gudmundsson1998}, \cite{Madsen_1998}, \cite{Zertchaninov1998}


Only bound-constrained problems are supported by this algorithm. 





\subsection{ISRES (Improved Stochastic Ranking Evolution Strategy)}
This is my implementation of the "Improved Stochastic Ranking Evolution Strategy" (ISRES) algorithm for nonlinearly-constrained global optimization (or at least semi-global; although it has heuristics to escape local optima, I'm not aware of a convergence proof), based on the method described in: \cite{Runarsson_2005}


It is a refinement of an earlier method described in: \cite{Runarsson_2000}



This is an independent implementation by S. G. Johnson (2009) based on the papers above. Runarsson also has his own Matlab implemention available from his web page here. 

The evolution strategy is based on a combination of a mutation rule (with a log-normal step-size update and exponential smoothing) and differential variation (a Nelder–Mead-like update rule). The fitness ranking is simply via the objective function for problems without nonlinear constraints, but when nonlinear constraints are included the stochastic ranking proposed by Runarsson and Yao is employed. The population size for ISRES defaults to 20×(n+1) in n dimensions, but this can be changed with the nlopt\_set\_stochastic\_population function. 

This method supports arbitrary nonlinear inequality and equality constraints in addition to the bound constraints, and is specified within NLopt as NLOPT\_GN\_ISRES. 





\section{NLOPT: Local derivative-free optimization}
\label{LocalDerivativeFreeOptimization}

Of these algorithms, only COBYLA currently supports arbitrary nonlinear inequality and equality constraints; the rest of them support bound-constrained or unconstrained problems only. (However, any of them can be applied to nonlinearly constrained problems by combining them with the augmented Lagrangian method below.) 


\subsection{COBYLA (Constrained Optimization BY Linear Approximations)}
This is a derivative of Powell's implementation of the COBYLA (Constrained Optimization BY Linear Approximations) algorithm for derivative-free optimization with nonlinear inequality and equality constraints, by M. J. D. Powell, described in: \cite{Powell_1994}


and reviewed in: \cite{Powell_1998}


It constructs successive linear approximations of the objective function and constraints via a simplex of n+1 points (in n dimensions), and optimizes these approximations in a trust region at each step. 

The original code itself was written in Fortran by Powell and was converted to C in 2004 by Jean-Sebastien Roy (js@jeannot.org) for the SciPy project. The version in NLopt was based on Roy's C version, downloaded from: 

http://www.jeannot.org/~js/code/index.en.html\#COBYLA 
NLopt's version is slightly modified in a few ways. First, we incorporated all of the NLopt termination criteria. Second, we added explicit support for bound constraints (although the original COBYLA could handle bound constraints as linear constraints, it would sometimes take a step that violated the bound constraints). Third, we allow COBYLA to increase the trust-region radius if the predicted improvement was approximately right and the simplex is OK, following a suggestion in the SAS manual for PROC NLP that seems to improve convergence speed. Fourth, we pseudo-randomize simplex steps in COBYLA algorithm, improving robustness by avoiding accidentally taking steps that don't improve conditioning (which seems to happen sometimes with active bound constraints); the algorithm remains deterministic (a deterministic seed is used), however. Also, we support unequal initial-step sizes in the different parameters (by the simple expedient of internally rescaling the parameters proportional to the initial steps), which is important when different parameters have very different scales. 

(The underlying COBYLA code only supports inequality constraints. Equality constraints are automatically transformed into pairs of inequality constraints, which in the case of this algorithm seems not to cause problems.) 

It is specified within NLopt as NLOPT\_LN\_COBYLA. 





\subsection{BOBYQA}
This is an algorithm derived from the BOBYQA subroutine of M. J. D. Powell, converted to C and modified for the NLopt stopping criteria. BOBYQA performs derivative-free bound-constrained optimization using an iteratively constructed quadratic approximation for the objective function. See: \cite{Powell2009}


(Because BOBYQA constructs a quadratic approximation of the objective, it may perform poorly for objective functions that are not twice-differentiable.) 

The NLopt BOBYQA interface supports unequal initial-step sizes in the different parameters (by the simple expedient of internally rescaling the parameters proportional to the initial steps), which is important when different parameters have very different scales. 

This algorithm, specified in NLopt as NLOPT\_LN\_BOBYQA, largely supersedes the NEWUOA algorithm below, which is an earlier version of the same idea by Powell. 




\subsection{NEWUOA + bound constraints}
This is an algorithm derived from the NEWUOA subroutine of M. J. D. Powell, converted to C and modified for the NLopt stopping criteria. I also modified the code to include a variant, NEWUOA-bound, that permits efficient handling of bound constraints. This algorithm is largely superseded by BOBYQA (above). 

The original NEWUOA performs derivative-free unconstrained optimization using an iteratively constructed quadratic approximation for the objective function. See: \cite{Powell_2004}



(Because NEWUOA constructs a quadratic approximation of the objective, it may perform poorly for objective functions that are not twice-differentiable.) 

The original algorithm is specified in NLopt as NLOPT\_LN\_NEWUOA, and only supports unconstrained problems. For bound constraints, my variant is specified as NLOPT\_LN\_NEWUOA\_BOUND. 

In the original NEWUOA algorithm, Powell solved the quadratic subproblems (in routines TRSAPP and BIGLAG) in a spherical trust region via a truncated conjugate-gradient algorithm. In my bound-constrained variant, we use the MMA algorithm for these subproblems to solve them with both bound constraints and a spherical trust region. In principle, we should also change the BIGDEN subroutine in a similar way (since BIGDEN also approximately solves a trust-region subproblem), but instead I just truncated its result to the bounds (which probably gives suboptimal convergence, but BIGDEN is called only very rarely in practice). 

Shortly after my addition of bound constraints to NEWUOA, Powell released his own version of NEWUOA modified for bound constraints as well as some numerical-stability and convergence enhancements, called BOBYQA. NLopt now incorporates BOBYQA as well, and it seems to largely supersede NEWUOA. 

Note: NEWUOA requires the dimension n of the parameter space to be $\geq 2$, i.e. the implementation does not handle one-dimensional optimization problems. 



\subsection{PRAXIS (Principal AXIS)}
"PRAXIS" gradient-free local optimization via the "principal-axis method" of Richard Brent, based on a C translation of Fortran code downloaded from Netlib: 

http://netlib.org/opt/praxis 
The original Fortran code was written by Richard Brent and made available by the Stanford Linear Accelerator Center, dated 3/1/73. The appropriate reference seems to be: \cite{Brent_1972}


Specified in NLopt as NLOPT\_LN\_PRAXIS 

This algorithm was originally designed for unconstrained optimization. In NLopt, bound constraints are "implemented" in PRAXIS by the simple expedient of returning infinity (Inf) when the constraints are violated (this is done automatically—you don't have to do this in your own function). This seems to work, more-or-less, but appears to slow convergence significantly. If you have bound constraints, you are probably better off using COBYLA or BOBYQA. 





\subsection{Nelder-Mead Simplex}
My implementation of almost the original Nelder-Mead simplex algorithm (specified in NLopt as NLOPT\_LN\_NELDERMEAD), as described in: \cite{Nelder_1965}


This method is simple and has demonstrated enduring popularity, despite the later discovery that it fails to converge at all for some functions (and examples may be constructed in which it converges to point that is not a local minimum). Anecdotal evidence suggests that it often performs well even for noisy and/or discontinuous objective functions. I would tend to recommend the Subplex method (below) instead, however. 

The main change compared to the 1965 paper is that I implemented explicit support for bound constraints, using essentially the method proposed in: \cite{Box_1965} 


and later reviewed in: \cite{Richardson_1973}


Whenever a new point would lie outside the bound constraints, Box advocates moving it "just inside" the constraints by some fixed "small" distance of 10−8 or so. I couldn't see any advantage to using a fixed distance inside the constraints, especially if the optimum is on the constraint, so instead I move the point exactly onto the constraint in that case. The danger with implementing bound constraints in this way (or by Box's method) is that you may collapse the simplex into a lower-dimensional subspace. I'm not aware of a better way, however. In any case, this collapse of the simplex is somewhat ameliorated by restarting, such as when Nelder-Mead is used within the Subplex algorithm below. 





\subsection{Sbplx (based on Subplex)}
This is my re-implementation of Tom Rowan's "Subplex" algorithm. As Rowan expressed a preference that other implementations of his algorithm use a different name, I called my implementation "Sbplx" (referred to in NLopt as NLOPT\_LN\_SBPLX). 

Subplex (a variant of Nelder-Mead that uses Nelder-Mead on a sequence of subspaces) is claimed to be much more efficient and robust than the original Nelder-Mead, while retaining the latter's facility with discontinuous objectives, and in my experience these claims seem to be true in many cases. (However, I'm not aware of any proof that Subplex is globally convergent, and perhaps it may fail for some objectives like Nelder-Mead; YMMV.) 

I used the description of Rowan's algorithm in his PhD thesis: \cite{Rowan_1990}

I would have preferred to use Rowan's original implementation, posted by him on Netlib: 

http://www.netlib.org/opt/subplex.tgz 
Unfortunately, the legality of redistributing or modifying this code is unclear, because it lacks anything resembling a license statement. After some friendly emails with Rowan in which he promised to consider providing a clear open-source/free-software license, I lost touch with him and his old email address now seems invalid. 

Since the algorithm is not too complicated, however, I just rewrote it. There seem to be slight differences between the behavior of my implementation and his (probably due to different choices of initial subspace and other slight variations, where his paper was ambiguous), but the number of iterations to converge on my test problems seems to be quite close (within ±10\% of the number of function evaluations for most problems). 

The only major difference between my implementation and Rowan's, as far as I can tell, is that I implemented explicit support for bound constraints (via the method in the Box paper as described above). This seems to be a big improvement in the case where the optimum lies against one of the constraints. 




\section{NLOPT: Local gradient-based optimization}
\label{LocalGradientBasedOptimization}

Of these algorithms, only MMA and SLSQP support arbitrary nonlinear inequality constraints, and only SLSQP supports nonlinear equality constraints; the rest support bound-constrained or unconstrained problems only. (However, any of them can be applied to nonlinearly constrained problems by combining them with the augmented Lagrangian method below.) 


\subsection{MMA (Method of Moving Asymptotes) and CCSA}
My implementation of the globally-convergent method-of-moving-asymptotes (MMA) algorithm for gradient-based local optimization, including nonlinear inequality constraints (but not equality constraints), specified in NLopt as NLOPT\_LD\_MMA, as described in: \cite{Svanberg_2002}


This is an improved CCSA ("conservative convex separable approximation") variant of the original MMA algorithm published by Svanberg in 1987, which has become popular for topology optimization. (Note: "globally convergent" does not mean that this algorithm converges to the global optimum; it means that it is guaranteed to converge to some local minimum from any feasible starting point.) 

At each point x, MMA forms a local approximation using the gradient of f and the constraint functions, plus a quadratic "penalty" term to make the approximations "conservative" (upper bounds for the exact functions). The precise approximation MMA forms is difficult to describe in a few words, because it includes nonlinear terms consisting of a poles at some distance from x (outside of the current trust region), almost a kind of Pade approximant. The main point is that the approximation is both convex and separable, making it trivial to solve the approximate optimization by a dual method. Optimizing the approximation leads to a new candidate point x. The objective and constraints are evaluated at the candidate point. If the approximations were indeed conservative (upper bounds for the actual functions at the candidate point), then the process is restarted at the new x. Otherwise, the approximations are made more conservative (by increasing the penalty term) and re-optimized. 

(If you contact Professor Svanberg, he has been willing in the past to graciously provide you with his original code, albeit under restrictions on commercial use or redistribution. The MMA implementation in NLopt, however, is completely independent of Svanberg's, whose code we have not examined; any bugs are my own, of course.) 

I also implemented another CCSA algorithm from the same paper, NLOPT\_LD\_CCSAQ: instead of constructing local MMA approximations, it constructs simple quadratic approximations (or rather, affine approximations plus a quadratic penalty term to stay conservative). This is the ccsa\_quadratic code. It seems to have similar convergence rates to MMA for most problems, which is not surprising as they are both essentially similar. However, for the quadratic variant I implemented the possibility of preconditioning: including a user-supplied Hessian approximation in the local model. It is easy to incorporate this into the proof in Svanberg's paper, and to show that global convergence is still guaranteed as long as the user's "Hessian" is positive semidefinite, and it practice it can greatly improve convergence if the preconditioner is a good approximation for the real Hessian (at least for the eigenvectors of the largest eigenvalues). 



\subsection{SLSQP}
Specified in NLopt as NLOPT\_LD\_SLSQP, this is a sequential quadratic programming (SQP) algorithm for nonlinearly constrained gradient-based optimization (supporting both inequality and equality constraints), based on the implementation by Dieter Kraft and described in: 

\cite{Kraft_1988, Kraft_1994}

(I believe that SLSQP stands for something like "Sequential Least-Squares Quadratic Programming," because the problem is treated as a sequence of constrained least-squares problems, but such a least-squares problem is equivalent to a QP.) The algorithm optimizes successive second-order (quadratic/least-squares) approximations of the objective function (via BFGS updates), with first-order (affine) approximations of the constraints. 

The Fortran code was obtained from the SciPy project, who are responsible for obtaining permission to distribute it under a free-software (3-clause BSD) license. 

The code was modified for inclusion in NLopt by S. G. Johnson in 2010, with the following changes. The code was converted to C and manually cleaned up. It was modified to be re-entrant (preserving the reverse-communication interface but explicitly saving the state in a data structure). The reverse-communication interface was wrapped with an NLopt-style interface, with NLopt stopping conditions. The inexact line search was modified to evaluate the functions including gradients for the first step, since this removes the need to evaluate the function+gradient a second time for the same point in the common case when the inexact line search concludes after a single step; this is motivated by the fact that NLopt's interface combines the function and gradient computations. Since roundoff errors sometimes pushed SLSQP's parameters slightly outside the bound constraints (not allowed by NLopt), we added checks to force the parameters within the bounds. We fixed a bug in the LSEI subroutine (use of uninitialized variables) for the case where the number of equality constraints equals the dimension of the problem. The LSQ subroutine was modified to handle infinite lower/upper bounds (in which case those constraints are omitted). 

Note: Because the SLSQP code uses dense-matrix methods (ordinary BFGS, not low-storage BFGS), it requires $O(n^2)$ storage and $O(n^3)$ time in $n$ dimensions, which makes it less practical for optimizing more than a few thousand parameters



\subsection{Low-storage BFGS}
This algorithm in NLopt (specified by NLOPT\_LD\_LBFGS), is based on a Fortran implementation of the low-storage BFGS algorithm written by Prof. Ladislav Luksan, and graciously posted online under the GNU LGPL at: 

http://www.uivt.cas.cz/~luksan/subroutines.html 
The original L-BFGS algorithm, based on variable-metric updates via Strang recurrences, was described by the papers: 

\cite{Nocedal_1980} and \cite{Liu_1989}.


I converted Prof. Luksan's code to C with the help of f2c, and made a few minor modifications (mainly to include the NLopt termination criteria). 

One of the parameters of this algorithm is the number M of gradients to "remember" from previous optimization steps: increasing M increases the memory requirements but may speed convergence. NLopt sets M to a heuristic value by default, but this can be changed by the set\_vector\_storage function. 





\subsection{Preconditioned truncated Newton}
This algorithm in NLopt, is based on a Fortran implementation of a preconditioned inexact truncated Newton algorithm written by Prof. Ladislav Luksan, and graciously posted online under the GNU LGPL at: 

http://www.uivt.cas.cz/~luksan/subroutines.html 

NLopt includes several variations of this algorithm by Prof. Luksan. First, a variant preconditioned by the low-storage BFGS algorithm with steepest-descent restarting, specified as NLOPT\_LD\_TNEWTON\_PRECOND\_RESTART. Second, simplified versions NLOPT\_LD\_TNEWTON\_PRECOND (same without restarting), NLOPT\_LD\_TNEWTON\_RESTART (same without preconditioning), and NLOPT\_LD\_TNEWTON (same without restarting or preconditioning). 

The algorithms are based on the ones described by: \cite{Dembo_1982}

I converted Prof. Luksan's code to C with the help of f2c, and made a few minor modifications (mainly to include the NLopt termination criteria). 

One of the parameters of this algorithm is the number M of gradients to "remember" from previous optimization steps: increasing M increases the memory requirements but may speed convergence. NLopt sets M to a heuristic value by default, but this can be changed by the set\_vector\_storage function. 





\subsection{Shifted limited-memory variable-metric}
This algorithm in NLopt, is based on a Fortran implementation of a shifted limited-memory variable-metric algorithm by Prof. Ladislav Luksan, and graciously posted online under the GNU LGPL at: 

http://www.uivt.cas.cz/~luksan/subroutines.html 
There are two variations of this algorithm: NLOPT\_LD\_VAR2, using a rank-2 method, and NLOPT\_LD\_VAR1, using a rank-1 method. 

The algorithms are based on the ones described by: \cite{Vlcek_2006}



I converted Prof. Luksan's code to C with the help of f2c, and made a few minor modifications (mainly to include the NLopt termination criteria). 

One of the parameters of this algorithm is the number M of gradients to "remember" from previous optimization steps: increasing M increases the memory requirements but may speed convergence. NLopt sets M to a heuristic value by default, but this can be changed by the set\_vector\_storage function. 




\section{NLOPT: Augmented Lagrangian algorithm}
\label{AugmentedLagrangian}

\subsection{Implementation}
There is one algorithm in NLopt that fits into all of the above categories, depending on what subsidiary optimization algorithm is specified, and that is the augmented Lagrangian method described in: \cite{Conn_1991} and \cite{Birgin_2008}



This method combines the objective function and the nonlinear inequality/equality constraints (if any) in to a single function: essentially, the objective plus a "penalty" for any violated constraints. This modified objective function is then passed to another optimization algorithm with no nonlinear constraints. If the constraints are violated by the solution of this sub-problem, then the size of the penalties is increased and the process is repeated; eventually, the process must converge to the desired solution (if it exists). 

The subsidiary optimization algorithm is specified by the nlopt\_set\_local\_optimizer function, described in the NLopt Reference. (Don't forget to set a stopping tolerance for this subsidiary optimizer!) Since all of the actual optimization is performed in this subsidiary optimizer, the subsidiary algorithm that you specify determines whether the optimization is gradient-based or derivative-free. In fact, you can even specify a global optimization algorithm for the subsidiary optimizer, in order to perform global nonlinearly constrained optimization (although specifying a good stopping criterion for this subsidiary global optimizer is tricky). 

The augmented Lagrangian method is specified in NLopt as NLOPT\_AUGLAG. We also provide a variant, NLOPT\_AUGLAG\_EQ, that only uses penalty functions for equality constraints, while inequality constraints are passed through to the subsidiary algorithm to be handled directly; in this case, the subsidiary algorithm must handle inequality constraints (e.g. MMA or COBYLA). 

While NLopt uses an independent re-implementation of the Birgin and Martínez algorithm, those authors provide their own free-software implementation of the method as part of the TANGO project, and implementations can also be found in semi-free packages like LANCELOT. 

